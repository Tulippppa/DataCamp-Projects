{"cells":[{"source":"![Credit card being held in hand](credit_card.jpg)\n\nCommercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.\n\n### The Data\n\nThe data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.","metadata":{},"id":"35aebf2e-0635-4fef-bc9a-877b6a20fb13","cell_type":"markdown"},{"source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \n\n# Seeing first few rows\ncc_apps.head()","metadata":{"executionCancelledAt":null,"executionTime":38,"lastExecutedAt":1731093237591,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \n\n# Seeing first few rows\ncc_apps.head()","outputsMetadata":{"0":{"height":222,"type":"dataFrame"}},"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2"},"id":"6e86b1e8-a3fa-4b09-982f-795f218bd1a6","cell_type":"code","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"0","type":"string"},{"name":"1","type":"string"},{"name":"2","type":"number"},{"name":"3","type":"string"},{"name":"4","type":"string"},{"name":"5","type":"string"},{"name":"6","type":"string"},{"name":"7","type":"number"},{"name":"8","type":"string"},{"name":"9","type":"string"},{"name":"10","type":"integer"},{"name":"11","type":"string"},{"name":"12","type":"integer"},{"name":"13","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"0":["b","a","a","b","b"],"1":["30.83","58.67","24.50","27.83","20.17"],"2":[0,4.46,0.5,1.54,5.625],"3":["u","u","u","u","u"],"4":["g","g","g","g","g"],"5":["w","q","q","w","w"],"6":["v","h","h","v","v"],"7":[1.25,3.04,1.5,3.75,1.71],"8":["t","t","t","t","t"],"9":["t","t","f","t","f"],"10":[1,6,0,5,0],"11":["g","g","g","g","s"],"12":[0,560,824,3,0],"13":["+","+","+","+","+"],"index":[0,1,2,3,4]}},"total_rows":5,"truncation_type":null},"text/plain":"  0      1      2  3  4  5  6     7  8  9   10 11   12 13\n0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  g    0  +\n1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  g  560  +\n2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  g  824  +\n3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  g    3  +\n4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  s    0  +","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b</td>\n      <td>30.83</td>\n      <td>0.000</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.25</td>\n      <td>t</td>\n      <td>t</td>\n      <td>1</td>\n      <td>g</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>a</td>\n      <td>58.67</td>\n      <td>4.460</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>3.04</td>\n      <td>t</td>\n      <td>t</td>\n      <td>6</td>\n      <td>g</td>\n      <td>560</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>a</td>\n      <td>24.50</td>\n      <td>0.500</td>\n      <td>u</td>\n      <td>g</td>\n      <td>q</td>\n      <td>h</td>\n      <td>1.50</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>g</td>\n      <td>824</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b</td>\n      <td>27.83</td>\n      <td>1.540</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>3.75</td>\n      <td>t</td>\n      <td>t</td>\n      <td>5</td>\n      <td>g</td>\n      <td>3</td>\n      <td>+</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b</td>\n      <td>20.17</td>\n      <td>5.625</td>\n      <td>u</td>\n      <td>g</td>\n      <td>w</td>\n      <td>v</td>\n      <td>1.71</td>\n      <td>t</td>\n      <td>f</td>\n      <td>0</td>\n      <td>s</td>\n      <td>0</td>\n      <td>+</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":14}]},{"source":"# **Step 1: Inspecting our Data**\nThe first step before analyzing any dataset, is to inspect our data. We need to learn more about our data. We can see the type of data we have, our summary statistics, and check the missing values","metadata":{},"cell_type":"markdown","id":"05dbaa24-3150-4679-ac18-746a8865e9cc"},{"source":"# Getting data types\nprint(cc_apps.info())\n\n# Looking at all 17 rows to see if we are missing data\nprint(cc_apps.tail(17))\n\n# Getting summary stats\ncc_apps.describe()","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1731093237644,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Getting data types\nprint(cc_apps.info())\n\n# Looking at all 17 rows to see if we are missing data\nprint(cc_apps.tail(17))\n\n# Getting summary stats\ncc_apps.describe()","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":306,"type":"dataFrame"}}},"cell_type":"code","id":"dd4b8156-db4b-4610-b16c-f1aa392bccc2","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 690 entries, 0 to 689\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       690 non-null    object \n 1   1       690 non-null    object \n 2   2       690 non-null    float64\n 3   3       690 non-null    object \n 4   4       690 non-null    object \n 5   5       690 non-null    object \n 6   6       690 non-null    object \n 7   7       690 non-null    float64\n 8   8       690 non-null    object \n 9   9       690 non-null    object \n 10  10      690 non-null    int64  \n 11  11      690 non-null    object \n 12  12      690 non-null    int64  \n 13  13      690 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 75.6+ KB\nNone\n    0      1       2  3  4   5   6      7  8  9   10 11   12 13\n673  ?  29.50   2.000  y  p   e   h  2.000  f  f   0  g   17  -\n674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  g  246  -\n675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  g  237  -\n676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  g    3  -\n677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  g    1  -\n678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  g   50  -\n679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  g    0  -\n680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  g  364  -\n681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  g  537  -\n682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  g    2  -\n683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  g    3  -\n684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  s    0  -\n685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  g    0  -\n686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  g  394  -\n687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  g    1  -\n688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  g  750  -\n689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  g    0  -\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"string"},{"name":"2","type":"number"},{"name":"7","type":"number"},{"name":"10","type":"number"},{"name":"12","type":"number"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"2":[690,4.7587246377,4.9781632485,0,1,2.75,7.2075,28],"7":[690,2.2234057971,3.3465133593,0,0.165,1,2.625,28.5],"10":[690,2.4,4.8629400342,0,0,0,3,67],"12":[690,1017.3855072464,5210.1025983027,0,0,5,395.5,100000],"index":["count","mean","std","min","25%","50%","75%","max"]}},"total_rows":8,"truncation_type":null},"text/plain":"               2           7          10             12\ncount  690.000000  690.000000  690.00000     690.000000\nmean     4.758725    2.223406    2.40000    1017.385507\nstd      4.978163    3.346513    4.86294    5210.102598\nmin      0.000000    0.000000    0.00000       0.000000\n25%      1.000000    0.165000    0.00000       0.000000\n50%      2.750000    1.000000    0.00000       5.000000\n75%      7.207500    2.625000    3.00000     395.500000\nmax     28.000000   28.500000   67.00000  100000.000000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2</th>\n      <th>7</th>\n      <th>10</th>\n      <th>12</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>690.000000</td>\n      <td>690.000000</td>\n      <td>690.00000</td>\n      <td>690.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.758725</td>\n      <td>2.223406</td>\n      <td>2.40000</td>\n      <td>1017.385507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4.978163</td>\n      <td>3.346513</td>\n      <td>4.86294</td>\n      <td>5210.102598</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>0.165000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.750000</td>\n      <td>1.000000</td>\n      <td>0.00000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.207500</td>\n      <td>2.625000</td>\n      <td>3.00000</td>\n      <td>395.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>28.000000</td>\n      <td>28.500000</td>\n      <td>67.00000</td>\n      <td>100000.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":15}],"execution_count":15},{"source":"# **Step 2: Splitting Data**\n\nAfter inspecting our dataset, we can finally split our data to create a training and testing dataset. We will need these different datasets to create our machine learning model.","metadata":{},"cell_type":"markdown","id":"14aeca87-343c-434b-8467-05744fe09595"},{"source":"# Importing splitting method\nfrom sklearn.model_selection import train_test_split\n\n# Splitting into train and test\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size = .33, random_state = 42)","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1731093237694,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Importing splitting method\nfrom sklearn.model_selection import train_test_split\n\n# Splitting into train and test\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size = .33, random_state = 42)"},"cell_type":"code","id":"403f623f-0069-4eb8-bdb1-64775c0bac8b","outputs":[],"execution_count":16},{"source":"# **Step 3: Handling Missing Values**\n\nNow, after we have split our data we can finally handle our issues: the missing data. As we saw when we looked at our data, these missing values are labled with \"?\". The first step we can do to combat this is changing the ? to NaN values, then we can replace the Na values with the mean.  ","metadata":{},"cell_type":"markdown","id":"5639714e-e82a-4c7a-a48f-37bb49d146cc"},{"source":"# Filling in missing values for NaN\ncc_apps_train= cc_apps_train.replace('?', np.NaN)\ncc_apps_test= cc_apps_test.replace('?', np.NaN)\n\n# Checking to see if \"?\" was removed\nprint(cc_apps_train.tail(17))\nprint(cc_apps_test.tail(17))\n\n# Replacing NaNs with means\ncc_apps_train.fillna(cc_apps_train.mean(), inplace=True)\ncc_apps_test.fillna(cc_apps_train.mean(), inplace=True)\n\n# Count the number of NaNs in the datasets and print the counts to verify\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum()) \n\n# Checking to see what other missing variables we have\nprint(cc_apps_train.info())\nprint(cc_apps_test.info())","metadata":{"executionCancelledAt":null,"executionTime":74,"lastExecutedAt":1731093237768,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Filling in missing values for NaN\ncc_apps_train= cc_apps_train.replace('?', np.NaN)\ncc_apps_test= cc_apps_test.replace('?', np.NaN)\n\n# Checking to see if \"?\" was removed\nprint(cc_apps_train.tail(17))\nprint(cc_apps_test.tail(17))\n\n# Replacing NaNs with means\ncc_apps_train.fillna(cc_apps_train.mean(), inplace=True)\ncc_apps_test.fillna(cc_apps_train.mean(), inplace=True)\n\n# Count the number of NaNs in the datasets and print the counts to verify\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum()) \n\n# Checking to see what other missing variables we have\nprint(cc_apps_train.info())\nprint(cc_apps_test.info())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"b76df5a3-24a8-4380-b4c4-f6d0ef35eda7","outputs":[{"output_type":"stream","name":"stdout","text":"    0      1       2    3    4    5    6       7  8  9   10 11    12 13\n130  b  67.75   5.500    u    g    e    z  13.000  t  t   1  g     0  +\n663  b  32.08   4.000    y    p   cc    v   1.500  f  f   0  g     0  -\n99   a  28.50   1.000    u    g    q    v   1.000  t  t   2  g   500  -\n372  a  45.00   4.585    u    g    k    h   1.000  f  f   0  s     0  -\n87   b  25.67   2.210    y    p   aa    v   4.000  t  f   0  g     0  -\n458  b  36.17   5.500    u    g    i   bb   5.000  f  f   0  g   687  -\n330  b  20.42   0.000  NaN  NaN  NaN  NaN   0.000  f  f   0  p     0  -\n214  b  26.67   2.710    y    p   cc    v   5.250  t  t   1  g     0  +\n466  b  31.08   3.085    u    g    c    v   2.500  f  t   2  g    41  -\n121  b  25.67  12.500    u    g   cc    v   1.210  t  t  67  g   258  +\n614  a  38.33   4.415    u    g    c    v   0.125  f  f   0  g     0  -\n20   b  25.00  11.250    u    g    c    v   2.500  t  t  17  g  1208  +\n71   b  34.83   4.000    u    g    d   bb  12.500  t  f   0  g     0  -\n106  b  28.75   1.165    u    g    k    v   0.500  t  f   0  s     0  -\n270  b  37.58   0.000  NaN  NaN  NaN  NaN   0.000  f  f   0  p     0  +\n435  b  19.00   0.000    y    p   ff   ff   0.000  f  t   4  g     1  -\n102  b  18.67   5.000    u    g    q    v   0.375  t  t   2  g    38  -\n      0      1       2  3  4   5   6       7  8  9   10 11    12 13\n409    b  17.08   0.250  u  g   q   v   0.335  f  t   4  g     8  -\n588    b  26.67   1.750  y  p   c   v   1.000  t  t   5  g  5777  +\n177    a  26.08   8.665  u  g  aa   v   1.415  t  f   0  g   150  +\n449    b  20.00   7.000  u  g   c   v   0.500  f  f   0  g     0  -\n656    b  25.67   3.250  u  g   c   h   2.290  f  t   1  g    21  -\n83     a    NaN   3.500  u  g   d   v   3.000  t  f   0  g     0  -\n447    a  27.33   1.665  u  g  ff  ff   0.000  f  f   0  g     1  -\n349    b  27.83   1.500  u  g   w   v   2.250  f  t   1  g     3  -\n453  NaN  29.75   0.665  u  g   w   v   0.250  f  f   0  g     0  -\n79     b  21.50   9.750  u  g   c   v   0.250  t  f   0  g     0  -\n23     a  27.42  14.500  u  g   x   h   3.085  t  t   1  g    11  +\n15     b  36.67   4.415  y  p   k   v   0.250  t  t  10  g     0  +\n375    a  20.83   0.500  y  p   e  dd   1.000  f  f   0  g     0  -\n234    a  58.42  21.000  u  g   i  bb  10.000  t  t  13  g  6700  +\n644    b  36.17   0.420  y  p   w   v   0.290  f  f   0  g     2  -\n271    b  32.33   2.500  u  g   c   v   1.250  f  f   0  g     0  -\n311    b  19.00   1.750  y  p   c   v   2.335  f  f   0  g     6  -\n0     8\n1     5\n2     0\n3     6\n4     6\n5     7\n6     7\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\ndtype: int64\n0     4\n1     7\n2     0\n3     0\n4     0\n5     2\n6     2\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\ndtype: int64\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 462 entries, 382 to 102\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       454 non-null    object \n 1   1       457 non-null    object \n 2   2       462 non-null    float64\n 3   3       456 non-null    object \n 4   4       456 non-null    object \n 5   5       455 non-null    object \n 6   6       455 non-null    object \n 7   7       462 non-null    float64\n 8   8       462 non-null    object \n 9   9       462 non-null    object \n 10  10      462 non-null    int64  \n 11  11      462 non-null    object \n 12  12      462 non-null    int64  \n 13  13      462 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 54.1+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 228 entries, 286 to 311\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       224 non-null    object \n 1   1       221 non-null    object \n 2   2       228 non-null    float64\n 3   3       228 non-null    object \n 4   4       228 non-null    object \n 5   5       226 non-null    object \n 6   6       226 non-null    object \n 7   7       228 non-null    float64\n 8   8       228 non-null    object \n 9   9       228 non-null    object \n 10  10      228 non-null    int64  \n 11  11      228 non-null    object \n 12  12      228 non-null    int64  \n 13  13      228 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 26.7+ KB\nNone\n"}],"execution_count":17},{"source":"As seen in our data,we still have a number of missing values. This happens to be because some of these variables are not numerical. We can combat this by first changing all the NaNs to a numerical value, by just replacing it with the most frequest value.","metadata":{},"cell_type":"markdown","id":"af2cee7e-1587-4413-babc-256ff43d15be"},{"source":"# Iterate over each column of cc_apps_train\nfor col in cc_apps_train.columns:\n    # Check if the column is of object type\n    if cc_apps_train[col].dtypes == 'object':\n        # Impute with the most frequent value\n        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n\n# Count the number of NaNs in the dataset and print the counts to verify\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum())\n\n# checking data types\nprint(cc_apps_train.info())\nprint(cc_apps_test.info())","metadata":{"executionCancelledAt":null,"executionTime":58,"lastExecutedAt":1731093237826,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Iterate over each column of cc_apps_train\nfor col in cc_apps_train.columns:\n    # Check if the column is of object type\n    if cc_apps_train[col].dtypes == 'object':\n        # Impute with the most frequent value\n        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n\n# Count the number of NaNs in the dataset and print the counts to verify\nprint(cc_apps_train.isnull().sum())\nprint(cc_apps_test.isnull().sum())\n\n# checking data types\nprint(cc_apps_train.info())\nprint(cc_apps_test.info())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"044f034d-5774-42bf-b268-fb14777098f1","outputs":[{"output_type":"stream","name":"stdout","text":"0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\ndtype: int64\n0     0\n1     0\n2     0\n3     0\n4     0\n5     0\n6     0\n7     0\n8     0\n9     0\n10    0\n11    0\n12    0\n13    0\ndtype: int64\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 462 entries, 382 to 102\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       462 non-null    object \n 1   1       462 non-null    object \n 2   2       462 non-null    float64\n 3   3       462 non-null    object \n 4   4       462 non-null    object \n 5   5       462 non-null    object \n 6   6       462 non-null    object \n 7   7       462 non-null    float64\n 8   8       462 non-null    object \n 9   9       462 non-null    object \n 10  10      462 non-null    int64  \n 11  11      462 non-null    object \n 12  12      462 non-null    int64  \n 13  13      462 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 54.1+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 228 entries, 286 to 311\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       228 non-null    object \n 1   1       228 non-null    object \n 2   2       228 non-null    float64\n 3   3       228 non-null    object \n 4   4       228 non-null    object \n 5   5       228 non-null    object \n 6   6       228 non-null    object \n 7   7       228 non-null    float64\n 8   8       228 non-null    object \n 9   9       228 non-null    object \n 10  10      228 non-null    int64  \n 11  11      228 non-null    object \n 12  12      228 non-null    int64  \n 13  13      228 non-null    object \ndtypes: float64(2), int64(2), object(10)\nmemory usage: 26.7+ KB\nNone\n"}],"execution_count":18},{"source":"# **Step 4: Preprocessing Data**\n\nNow, we solved our missing values proble. However, as seen above, our data is still not all numeric. Before we can build our machine learning model, we need to preprocess our data. We need to change it to numeric","metadata":{},"cell_type":"markdown","id":"f89c6e34-1f3c-4c1a-a3fd-0f0e8b1a8991"},{"source":"# Convert non-numeric data in  train and test\ncc_apps_train = pd.get_dummies(cc_apps_train)\ncc_apps_test = pd.get_dummies(cc_apps_test)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)\n\n# checking data types\nprint(cc_apps_train.info())\nprint(cc_apps_test.info())","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1731093237877,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Convert non-numeric data in  train and test\ncc_apps_train = pd.get_dummies(cc_apps_train)\ncc_apps_test = pd.get_dummies(cc_apps_test)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)\n\n# checking data types\nprint(cc_apps_train.info())\nprint(cc_apps_test.info())","outputsMetadata":{"0":{"height":269,"type":"stream"}}},"cell_type":"code","id":"ff46e8ea-3b83-4673-b6b1-c8f7b601f194","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 462 entries, 382 to 102\nColumns: 334 entries, 2 to 13_-\ndtypes: float64(2), int64(2), uint8(330)\nmemory usage: 166.9 KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 228 entries, 286 to 311\nColumns: 334 entries, 2 to 13_-\ndtypes: float64(2), int64(179), uint8(153)\nmemory usage: 358.3 KB\nNone\n"}],"execution_count":19},{"source":"Now, we have successfully changed all our data to numeric! We can finally start creating our model! To do so, we will need to rescale our data to improve our prediction accuracy.","metadata":{},"cell_type":"markdown","id":"1a14ff4e-a6b7-4bf3-a8c8-c720c871e39c"},{"source":"# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Creating X train and y train \nX_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\nX_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1731093237926,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Creating X train and y train \nX_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\nX_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)"},"cell_type":"code","id":"04e11ec1-446d-4864-b245-ece8659c38db","outputs":[],"execution_count":20},{"source":"# **Step 5: Instantiate and fitting Our Model**\n\nBecause we finally preprocessed and cleaned our data, we can build our model.","metadata":{},"cell_type":"markdown","id":"edaa22db-626c-429c-bc6b-f0e83a5c6380"},{"source":"# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate a LogisticRegression classifier\nlogreg = LogisticRegression()\n\n# Fit to rain \nlogreg.fit(rescaledX_train,y_train)","metadata":{"executionCancelledAt":null,"executionTime":471,"lastExecutedAt":1731093238397,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Instantiate a LogisticRegression classifier\nlogreg = LogisticRegression()\n\n# Fit to rain \nlogreg.fit(rescaledX_train,y_train)"},"cell_type":"code","id":"97cb8ec9-600f-401a-92d9-e15b4f2fceca","outputs":[{"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{},"execution_count":21}],"execution_count":21},{"source":"# Import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Get the accuracy score of logreg model and print it\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test,y_test))\n\n# Print the confusion matrix of the logreg model\nconfusion_matrix(y_test,y_pred)","metadata":{"executionCancelledAt":null,"executionTime":101,"lastExecutedAt":1731093238498,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Get the accuracy score of logreg model and print it\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test,y_test))\n\n# Print the confusion matrix of the logreg model\nconfusion_matrix(y_test,y_pred)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"ecae46cb-123e-4d4e-8d85-d7616ed36a1c","outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy of logistic regression classifier:  1.0\n"},{"output_type":"execute_result","data":{"text/plain":"array([[103,   0],\n       [  0, 125]])"},"metadata":{},"execution_count":22}],"execution_count":22},{"source":"# **Step 6: Grid Search**\n\nNow, our next step is we can perform a grid search. A grid search is often used to explore all possible combinations of different hyperparameters within a specified range. In the end, this leads to an improved accuracy and better model performance because it tunes the paramaters itself. ","metadata":{},"cell_type":"markdown","id":"775d6f0e-a0c4-49a1-949b-2458e45da0b4"},{"source":"#Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001 ,0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)","metadata":{"executionCancelledAt":null,"executionTime":140,"lastExecutedAt":1731093238638,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001 ,0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)"},"cell_type":"code","id":"24a92f85-6323-4407-81c5-39c2c3a99ae1","outputs":[],"execution_count":23},{"source":"# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test,y_test))\n","metadata":{"executionCancelledAt":null,"executionTime":3854,"lastExecutedAt":1731093242492,"lastExecutedByKernel":"db15a840-f61e-4fa2-b089-491a0a5ec2c2","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test,y_test))\n","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"cell_type":"code","id":"f355661c-8e86-47cf-9085-21a9e276a66d","outputs":[{"output_type":"stream","name":"stdout","text":"Best: 1.000000 using {'max_iter': 100, 'tol': 0.01}\nAccuracy of logistic regression classifier:  1.0\n"}],"execution_count":24},{"source":"Here, we see that our accuracy for the model is 1. The output is both showing the best hyperparameters and indicating that, with these settings, the model achieved perfect accuracy. Max_iter 100 references the maximum number of iterations the logistic regression model will run to find the optimal solution.\ntol 0.01 references the tolerance level for convergence; the model stops when the change between iterations is smaller than this threshold.","metadata":{},"cell_type":"markdown","id":"b736fe52-33ac-4642-ac54-c2a40d521c40"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}